---
layout: post
title: "Diffusion-III DDIM"
date: 2025-10-06
math: true
published: true
---

We go over DDIM [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502) by J. Song et al 2020.



#### Non-Markovian


Recall that [DDPM](https://ziluma.github.io/2025/09/28/diffusion-1.html) assumes forward and backward processes are Markov. DDPM prescribes $p(x_t\mid x_{t-1})$ and then obtains $p(x_t\mid x_0)$ and $p(x_{t-1}\mid x_{t},x_0)$.


DDIM assumes that the forward process is Markov <i>conditioned on $X_0$</i> (instead of fully Markov) and assumes

$$
    X_t\mid X_0 \sim \mathcal{N}(\alpha_t X_0, \beta_t^2 I)
$$

where $\alpha_t^2+\beta_t^2=1.$ 
Why? $\alpha_t=\bar\alpha^{\rm DDPM}_t,\beta_t=\bar\beta_t^{\rm DDPM}$ using notations from [DDPM](https://ziluma.github.io/2025/09/28/diffusion-1.html).

We still prescribe $p(x_{t-1}\mid x_t,x_0)$ in a closed form:

$$
    X_{t-1}\mid X_t,X_0 \sim \mathcal{N}(\tilde\mu_t(X_t,X_0),\sigma_t^2I)
$$

where $\tilde\mu_t(X_t,X_0)=a_tX_t+b_tX_0$ TBD.
We are free to choose such coefficients as long as

$$
    p(x_{t-1}\mid x_0) = \int p(x_{t-1}\mid x_t,x_0)p(x_t\mid x_0)\,dx_t.
$$

Since

$$
    X_t=\alpha_t X_0 + \beta_t\varepsilon_t,\quad 
    X_{t-1}=a_tX_t+b_t X_0 + \sigma_t\bar\varepsilon_t,
$$

where $\varepsilon_t,\bar\varepsilon_t$ are standard Gaussians and we assume $\varepsilon_t,\bar\varepsilon_t$ are independet.

$$
\alpha_{t-1}X_0+\beta_{t-1}\varepsilon_{t-1}
=X_{t-1}
    = a_t(\alpha_tX_0+\beta_t\varepsilon_t)+b_t X_0 + \sigma_t\bar\varepsilon_t.
$$

Then

$$
\alpha_ta_t+b_t=\alpha_{t-1},\quad 
\beta_{t-1}^2=a_t^2\beta_t^2+\sigma_t^2.
$$

3 unknowns for 2 equtions. Say we are free to choose $\sigma_t$.

$$
a_t = \beta_t^{-1}\sqrt{\beta_{t-1}^2-\sigma_t^2},\quad 
b_t = \alpha_{t-1}-a_t\alpha_t.
$$

So, for $t>1$,

$$
\boxed{X_{t-1}\mid X_t,X_0\sim 
\mathcal{N}\left(\alpha_{t-1}X_0 + a_t(X_t-\alpha_tX_0),\sigma_t^2I \right).}
$$

If choosing $\sigma_t^2=\frac{\beta_{t-1}^2}{\beta_t^2}(1-\frac{\alpha_t^2}{\alpha_{t-1}^2})$ or $\tilde\beta_t^2$ from [DDPM](https://ziluma.github.io/2025/09/28/diffusion-1.html), then $\tilde\mu_t$ coincides with that of DDPM.
#### Hit-n-run

We can model $q_\theta(x_{t-1}\mid x_t)$ by $p(x_{t-1}\mid x_t,x_0)$.
First make a guess $f_t^\theta(x_t)$ of $x_0$ based on $x_t$, and then plug this into $p(x_{t-1}\mid x_t,x_0)$ to improve the guess. Define 

$$
\boxed{
q_\theta(x_{t-1}\mid x_t) := p(x_{t-1}\mid x_t,x_0=f_t^\theta(x_t)),\quad
q_\theta(x_0\mid x_1) := \mathcal{N}(x_0\mid f_1^\theta(x_1),\sigma_1^2I).}
$$

Then same as [DDPM](https://ziluma.github.io/2025/09/28/diffusion-1.html),

$$
\begin{align*}
&D_{\rm KL}(p(x_{0:T})\,\|\, q(x_{0:T}))
= \int p_{\rm data}(x_0)\,dx_0
\int p(x_{1:T}\mid x_0)\log\frac{p(x_{1:T}\mid x_0)p_{\rm data}(x_0)}{q(x_{0:T})}\, dx_{1:T}\\ 
&= C
- \sum_{t=1}^T\mathbf{E}_{p_{\rm data}} \int p(x_{t}\mid x_{t-1},x_0)p(x_{t-1}\mid x_0)
     \log {q(x_{t-1}\mid x_t)}
\,d x_{t-1}dx_t \\ 
&= C' -\sum_{t=1}^T\mathbf{E}_{p_{\rm data}} D_{\rm KL}(p(x_{t-1}\mid x_t,x_0)\,\|\, q(x_{t-1}\mid x_t) )\\
&=: C'+\sum_{t=1}^T L_t.
\end{align*}
$$

For $t>1$, (see details in [DDPM](https://ziluma.github.io/2025/09/28/diffusion-1.html))

$$
\begin{align*}
L_t&= C + \frac{1}{2\sigma_t^2} \mathbf{E}\left|\tilde\mu_t(X_t,X_0)-\tilde\mu_t(X_t,f^\theta_t(X_t))\right|^2 \\ 
&=C + \frac{b_t^2}{2\sigma_t^2} \mathbf{E}\left|X_0-f^\theta_t(\alpha_tX_0+\beta_t\varepsilon_t)\right|^2
\end{align*}
$$

where $X_0\sim p_{\rm data},X_t=\alpha_tX_0+\beta_t\varepsilon_t,\varepsilon_t\sim \mathcal{N}.$

Similar to DDPM, inspired by $X_t=\alpha_tX_0+\beta_t\varepsilon_t$, define

$$
\boxed{
X_t = \alpha_t f_t^\theta(X_t) + \beta_t\varepsilon_t^\theta(X_t).}
$$

Then 

$$
\begin{align*}
L_t &= C + \frac{b_t^2}{2\sigma_t^2} \mathbf{E}\left|\alpha_t^{-1}(X_t-\beta_t\varepsilon_t)-f^\theta_t(X_t)\right|^2 \\ 
&= C + \frac{b_t^2\beta_t^2}{2\sigma_t^2\alpha_t^2} 
\mathbf{E}\left|\varepsilon_t - \varepsilon_t^\theta(\alpha_tX_0 + \beta\varepsilon_t)\right|^2,
\end{align*}
$$

almost the same as DDPM.

In practice, the DDIM objective is

$$
\mathbf{E}_{t\sim U, \varepsilon\sim\mathcal{N},X_0\sim p_{\rm data}}
\gamma_t\left|\varepsilon - \varepsilon_t^\theta(\alpha_tX_0+\beta_t\varepsilon)\right|^2.
$$

$\gamma_t\equiv 1$ in DDPM.

### Sampling

After training $\varepsilon_t^\theta$ and thus $f_t^\theta$,
we use $q_\theta(x_{t-1}\mid x_t)$ to sample.
Sample $X_T\sim\mathcal{N}$. For $t>1$, 

$$
\begin{align*}
    X_{t-1} 
    &= a_t X_t 
    + (\alpha_{t-1}-a_t\alpha_t)f_t^\theta(X_t) + \sigma_t \bar\varepsilon_t\\ 
    &= \boxed{\alpha_{t-1}f_t^\theta(X_t) + a_t\beta_t \varepsilon_t^\theta(X_t)
    + \sigma_t\bar\varepsilon_t}
\end{align*}
$$

where $\bar\varepsilon_t\sim\mathcal{N}$. Finally,

$$
\boxed{X_0 = f_1^\theta(X_1) + \sigma_1 \bar\varepsilon_1.}
$$
