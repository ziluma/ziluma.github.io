---
layout: post
title: "DDPM-I basics"
date: 2025-09-28
math: true
published: true
---

We go over **DDPM** (Denoising Diffusion Probabilistic Model), a popular generative model introduced by [Ho et al. 2020](https://arxiv.org/abs/2006.11239). The math was mainly deduced by [Sohl-Dickstein et al. 2015](https://arxiv.org/abs/1503.03585). I partly follow [this post](https://spaces.ac.cn/archives/9119) by 苏剑林


**Goal**: generate samples $\sim p_{\rm data}(x).$

**Idea**: $X_0\sim p_{\rm data}\to X_1\to \cdots \to$ $$X_T\approx \mathcal{N}(0,I).$$ 

- <u>Forward/encoding process</u>: $p(x_t\mid x_{t-1})$ gradually destroys structure and adds Gaussian noise with variance schedules $\beta_t^2$, not trainable.

- <u>Reverse/decoding process</u>: $q_\theta(x_{t-1}\mid x_t)$ restores structure and recovers $p_{\rm data}$ with $\theta$ to be trained.

- Can be understood as a sequential [VAE](https://ziluma.github.io/2025/09/25/VAE1.html).

- Note: In the DDPM paper, $\beta_t^{\rm DDPM}=\beta_t^2$. Also, $p\leftrightarrow q$ to coincide with notations from VAE: $q$ is decoding.

**Assumptions**:

- Forward and reverse processes are Markov.

- $$X_{t-1} \mid X_t \sim \mathcal{N}(\mu_\theta(X_t,t), \sigma_t^2I).$$

- $X_t=\sqrt{1-\beta_t^2} X_{t-1}+\beta_t\varepsilon_t,$ where $\varepsilon_t\sim \mathcal{N}(0,I).$ 

**A direct consequence** is that $q(x_t\mid x_0)$ is an explicit Gaussian: writing $\alpha_t=\sqrt{1-\beta_t^2}$,

$$
\begin{align*}
X_t &= \alpha_t X_{t-1}+\beta_t\varepsilon_t 
= \alpha_t\alpha_{t-1}X_{t-2} +\beta_t\varepsilon_t  + \alpha_t\beta_{t-1}\varepsilon_{t-1}\\ 
&= \alpha_t\cdots\alpha_1 X_0 
+ \sum_{s=1}^t \beta_s \alpha_{s+1}\cdots\alpha_t\varepsilon_s.
\end{align*}
$$

Since 
$$
{\rm Var} \sum_{s=1}^t \beta_s \alpha_{s+1}\cdots\alpha_t\varepsilon_s
= \sum_{s=1}^t \beta_s^2\alpha_{s+1}^2\cdots\alpha_t^2
= 1-\alpha_1^2\cdots\alpha_t^2,
$$
if $\bar\alpha_t=\alpha_1\cdots\alpha_t,\bar\beta_t=\sqrt{1-\bar\alpha_t^2}$, then 

$$
X_t \mid X_0 \sim \mathcal{N}(\bar\alpha_t X_0, \bar\beta_t^2 I).
$$

### Loss

Writing $z=x_{1:T},$ as in [VAE](https://ziluma.github.io/2025/09/25/VAE1.html),
we complete the joint $p(x_{0:T})$ by marginal $p_{\rm data}(x_0).$
The KL of joints is

$$
\begin{align*}
&D_{\rm KL}(p(x_{0:T})\,\|\, q(x_{0:T}))
= \int p_{\rm data}(x_0)\,dx_0
\int p(z\mid x_0)\log\frac{p(z\mid x_0)p_{\rm data}(x_0)}{q(x_0,z)}\, dz\\ 
&= -H(p_{\rm data})
+ \mathbf{E}_{p_{\rm data}} \int p(x_T\mid x_{T-1})\cdots p(x_1\mid x_0)
\log \frac{p(x_T\mid x_{T-1})\cdots p(x_1\mid x_0)}{q(x_0\mid x_1)\cdots q(x_{T-1}\mid x_T)}
\,d x_{1:T}.
\end{align*}
$$

**Note**: This is different from VAE or EM as $p$ goes first in KL and KL is asymmetric. 

As the forward process $p$ is not trainable, we can drop some terms:

$$
\begin{align*}
D_{\rm KL}(p(x_{0:T})\,\|\, q(x_{0:T}))
=C
- \mathbf{E}_{p_{\rm data}} \int p(x_T\mid x_{T-1})\cdots p(x_1\mid x_0)
    \sum_{t=1}^T \log q(x_{t-1}\mid x_t)
\,d x_{1:T}
\end{align*}
$$

Call each term $L_t$ in the sum. By marginization and computations above,

$$
\begin{align*}
L_t&= -\mathbf{E}_{p_{\rm data}}\int p(x_t\mid x_{t-1})p(x_{t-1}\mid x_0) \log q(x_{t-1}\mid x_t)\,d x_{t-1}dx_t\\ 
&= C + \frac{1}{2\sigma_t^2}\mathbf{E} \| X_{t-1}-\mu_\theta(X_{t},t)\|^2
\end{align*}
$$

where in $\mathbf{E}$,

$$
X_0\sim p_{\rm data},\quad X_{t-1}=\bar\alpha_{t-1}X_0 + \bar\beta_{t-1}\bar\varepsilon_{t-1},\quad 
X_t=\alpha_t X_{t-1}+\beta_t \varepsilon_t,
$$

for some independent standard Gaussians $\varepsilon_t,\bar\varepsilon_{t-1}.$

As $X_{t-1}=\alpha_t^{-1}(X_t-\beta_t\varepsilon_t)$, we write

$$
    \mu_\theta(x,t) =: \alpha_t^{-1}(x- \beta_t\varepsilon_\theta(x,t))
$$

Then we are really minimizing

$$
\frac{\beta_t^2}{\alpha_t^2\sigma_t^2} \mathbf{E}
\left\|
    \varepsilon_t - \varepsilon_\theta(X_t,t)
\right\|^2,
$$

where

$$
X_t = \bar\alpha_{t}X_0 + \alpha_t\bar\beta_{t-1}\bar\varepsilon_{t-1}+\beta_t \varepsilon_t.
$$

We can simplify the two noise terms. Since

$$
{\rm Var}(\alpha_t\bar\beta_{t-1}\bar\varepsilon_{t-1}+\beta_t \varepsilon_t) = \alpha_t^2(1-\bar\alpha_{t-1}^2)+\beta_t^2
=1-\bar\alpha_t^2 = \bar\beta_t^2,
$$

we change variables by a rotation:

$$
\bar\beta_t 
\begin{bmatrix}
\varepsilon\\
\delta
\end{bmatrix}
= \begin{bmatrix}
\beta_t& \alpha_t\bar\beta_{t-1}\\
-\alpha_t\bar\beta_{t-1} &\beta_t
\end{bmatrix}
\begin{bmatrix}
\varepsilon_{t}\\
\bar\varepsilon_{t-1}
\end{bmatrix}.
$$

Then $\varepsilon,\delta$ are independent standard Gaussians and 

$$
X_t=\bar\alpha_t X_0 + \bar\beta_t\varepsilon,\quad 
\bar\beta_t\varepsilon_t = \beta_t\varepsilon - \alpha_t\bar\beta_{t-1}\delta.
$$

The loss becomes

$$
\begin{align*}
\mathbf{E}
\left\|
    \varepsilon_t - \varepsilon_\theta(X_t,t)
\right\|^2
&=
\bar\beta_t^{-2} \mathbf{E}
\left\|
    \beta_t\varepsilon - \alpha_t\bar\beta_{t-1}\delta 
    - \varepsilon_\theta(\bar\alpha_t X_0 + \bar\beta_t\varepsilon,t)
\right\|^2\\
&= \bar\beta_t^{-2} \mathbf{E}
\left\|
    \beta_t\varepsilon  
    - \varepsilon_\theta(\bar\alpha_t X_0 + \bar\beta_t\varepsilon,t)
\right\|^2
\end{align*}
$$