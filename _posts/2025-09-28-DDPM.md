---
layout: post
title: "DDPM-I basics"
date: 2025-09-28
math: true
---

We go over **DDPM** (Denoising Diffusion Probabilistic Model), a popular generative model introduced by [Ho et al. 2020](https://arxiv.org/abs/2006.11239). The math was mainly deduced by [Sohl-Dickstein et al. 2015](https://arxiv.org/abs/1503.03585).


**Goal**: generate samples $\sim p_{\rm data}(x).$

**Idea**: $X_0\sim p_{\rm data}$, $$X_T\sim \mathcal{N}(0,I).$$ 

- <u>Forward/encoding process</u>: $p(x_t\mid x_{t-1})$ gradually destroys structure and adds Gaussian noise with variance schedules $\beta_t^2$, not trainable.

- <u>Reverse/decoding process</u>: $q_\theta(x_{t-1}\mid x_t)$ restores structure and recovers $p_{\rm data}$ with $\theta$ to be trained.

- Can be understood as a sequential [VAE](https://ziluma.github.io/2025/09/25/VAE1.html).

- Note: our $\beta_t$ is the square root of $\beta_t^{\rm DDPM}$ from the DDPM paper. Also, $p\leftrightarrow q$ to coincide with notations from VAE: $q$ is decoding.

**Assumptions**:

- Forward and reverse processes are Markov.

- $$X_{t-1} \mid X_t \sim \mathcal{N}(\mu_\theta(X_t,t), \sigma_t^2I).$$

- $X_t=\sqrt{1-\beta_t^2} X_{t-1}+\beta_t\varepsilon_t,$ where $\varepsilon_t\sim \mathcal{N}(0,I).$ 

A direct consequence is that $q(x_t\mid x_0)$ is an explicit Gaussian: writing $\alpha_t=\sqrt{1-\beta_t^2}$,

$$
\begin{align*}
X_t &= \alpha_t X_{t-1}+\beta_t\varepsilon_t 
= \alpha_t\alpha_{t-1}X_{t-2} +\beta_t\varepsilon_t  + \alpha_t\beta_{t-1}\varepsilon_{t-1}\\ 
&= \alpha_t\cdots\alpha_1 X_0 
+ \sum_{s=1}^t \beta_s \alpha_{s+1}\cdots\alpha_t\varepsilon_s.
\end{align*}
$$

Since 
$$
{\rm Var} \sum_{s=1}^t \beta_s \alpha_{s+1}\cdots\alpha_t\varepsilon_s
= \sum_{s=1}^t \beta_s^2\alpha_{s+1}^2\cdots\alpha_t^2
= 1-\alpha_1^2\cdots\alpha_t^2,
$$
if $\bar\alpha_t=\alpha_1\cdots\alpha_t$, then 

$$
X_t \mid X_0 \sim \mathcal{N}(\bar\alpha_t X_0, (1-\bar\alpha_t^2) I).
$$

### Log-likelihood 

$$
\begin{align*}
\log q(x_0:x_T)
\end{align*}
$$

