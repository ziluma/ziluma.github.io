---
layout: post
title: "VAE-II another EM?"
date: 2025-09-27
math: true
---

In my last [note on VAE](https://ziluma.github.io/2025/09/25/VAE1.html), the ELBO reminds me of the [EM (expectation-maximization)](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) algo, so it's worth exploring how they are related.


<!-- We use Jensen a lot, which says for any convex $\phi$,
$$
    \phi(\mathbf{E}X)\le \mathbf{E}[\phi(X)].
$$
If $\phi$ is strictly convex, the equality holds iff. $X=\mathbf{E}X$ a.s.
The proof is essentially the base case: $ \phi(\frac{a+b}{2})\le \frac{1}{2}(\phi(a)+\phi(b)). $ -->

Consider a variable $X$ with some latent $Z$ which secretely affects $X$.
We adopt a view [here](https://deepgenerativemodels.github.io/notes/vae/).
Our goal is to model $X$ from a parametrized family $p_\theta(x),$ so a point estimation. Given data 
$\lbrace x^{(i)} \rbrace_{i=1}^N,$

$$
\begin{align*}
D_{\rm KL}(p_{\rm data}\| p_\theta)
&= \int p_{\rm data} \log \frac{p_{\rm data}}{p_\theta}
= -H(p_{\rm data}) - \int p_{\rm data}\log p_\theta \\
&\approx  -H(p_{\rm data}) - \frac{1}{N}\sum_{i=1}^N\log p_\theta(x^{(i)}).
\end{align*}
$$

Minimizing such KL is equivalent to maximizing $\ell(\theta)=\sum \log p_\theta(x^{(i)})$, the usual maximum likelihood estimator (MLE).

### EM recap

Let's recall EM briefly. See [Ng's notes](https://cs229.stanford.edu/notes2020spring/cs229-notes8.pdf) for more details.

It is often infeasible to directly find MLE, but here we assume $p_\theta(x,z)$ is more feasible. Given any distribution $q$ on the $Z$-space,

$$
\begin{align*}
\log p_\theta(x) 
&= \mathbf{E}_{z\sim q} \log \frac{p_\theta(x,z)}{p_\theta(z\mid x)}
= \mathbf{E}_{q} \log \frac{p_\theta(x,z)}{q(z)}+
\mathbf{E}_{q}\log\frac{q(z)}{p_\theta(z\mid x)}\\ 
&=: \mathcal{L}(\theta,q\mid x) + D_{\rm KL}(q(z)\,\|\, p_\theta(z|x))\\ 
&\ge \mathcal{L}(\theta,q \mid x),
\end{align*}
$$

where the equality holds iff. $q(z)=p_\theta(z\mid x).$ 
The computation is the same as in VAE.

EM algorithm is an alternation optimization method. Start with an initla guess $\theta^0$.
At step $k$, 
compute two things: 

E-step:

$$
    Q(\theta,\theta^k) := \sum_{i} \mathcal{L}(\theta,p_{\theta^k}(z\mid x^{(i)})\mid x^{(i)}).
$$

M-step:

$$
    \theta^{k+1}\in {\rm arg}\max Q(\theta,\theta^k).
$$

The E-step is really choosing the maximizer of $\mathcal{L}(\theta^k,\cdot \mid x^{(i)})$, i.e. $p_{\theta^k}(z\mid x^{(i)}).$


$$
Q(\theta^k,\theta^k)=\ell(\theta^k),\quad 
Q(\theta,\theta^k)\le \ell(\theta).
$$ 

So $Q(\cdot,\theta^k)$ is a lower barrier of $\ell$ at $\theta^k$ (in analysts' term). 

EM always increases likelihood:

$$
    \ell(\theta^{k+1})\ge Q(\theta^{k+1},\theta^{k})
    \ge Q(\theta^k,\theta^k) = \ell(\theta^k). 
$$

EM has nice convergence properties by [J. Wu](https://projecteuclid.org/journals/annals-of-statistics/volume-11/issue-1/On-the-Convergence-Properties-of-the-EM-Algorithm/10.1214/aos/1176346060.full) and [my joint work](https://www.sciencedirect.com/science/article/abs/pii/S0020025518304535) etc.

### VAE

In E-step, EM maximizes $\mathcal{L}(\theta^k,\cdot\mid x^{(i)})$ without restriction, while VAE searches among $q_\phi(z\mid x^{(i)})$ with a prescribed form of Gaussian.
Given a candidate $q^\star(z\mid x^{(i)})$, EM in M-step maximizes $\mathcal{L}(\cdot,q^\star\mid x^{(i)})$, while VAE optimizes $\theta$ in the decoder $p_\theta(x\mid \tilde z)$ with $z\sim q^\star$.
VAE does not split into two steps but optimizes all together.