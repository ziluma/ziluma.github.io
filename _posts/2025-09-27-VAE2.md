---
layout: post
title: "VAE-II another EM?"
date: 2025-09-27
math: true
---

The evidence lower bound reminds me of the EM (expectation-maximization) algorithm. So it is worth exploring their relations.

Let's recall EM briefly. See [Ng's notes](https://cs229.stanford.edu/notes2020spring/cs229-notes8.pdf) for more details.

<!-- We use Jensen a lot, which says for any convex $\phi$,
$$
    \phi(\mathbf{E}X)\le \mathbf{E}[\phi(X)].
$$
If $\phi$ is strictly convex, the equality holds iff. $X=\mathbf{E}X$ a.s.
The proof is essentially the base case: $ \phi(\frac{a+b}{2})\le \frac{1}{2}(\phi(a)+\phi(b)). $ -->

Consider a variable $X$ with some latent $Z$ which secretely affects $X$.
We adopt a view [here](https://deepgenerativemodels.github.io/notes/vae/).
Our goal is to model $X$ from a parametrized family $p_\theta(x),$ so a point estimation. Given data 
$\lbrace x^{(i)} \rbrace_{i=1}^N,$

$$
\begin{align*}
D_{\rm KL}(p_{\rm data}\| p_\theta)
&= \int p_{\rm data} \log \frac{p_{\rm data}}{p_\theta}
= -H(p_{\rm data}) - \int p_{\rm data}\log p_\theta \\
&\approx  -H(p_{\rm data}) - \frac{1}{N}\sum_{i=1}^N\log p_\theta(x^{(i)}).
\end{align*}
$$

Minimizing such KL is equivalent to maximizing $\ell(\theta)=\sum \log p_\theta(x^{(i)})$, the usual maximum likelihood estimator (MLE).

It is often infeasible to directly find MLE, but here we assume $p_\theta(x,z)$ is more feasible. Given any distribution $q$ on the $Z$-space,

$$
\begin{align*}
\log p_\theta(x) 
&= \mathbf{E}_{z\sim q} \log \frac{p_\theta(x,z)}{p_\theta(z\mid x)}
= \mathbf{E}_{q} \log \frac{p_\theta(x,z)}{q(z)}+
\mathbf{E}_{q}\log\frac{q(z)}{p_\theta(z\mid x)}\\ 
&=: \mathcal{L}(\theta,q\mid x) + D_{\rm KL}(q(z)\,\|\, p_\theta(z|x))\\ 
&\ge \mathcal{L}(\theta,q \mid x),
\end{align*}
$$

where the equality holds iff. $q(z)=p_\theta(z\mid x).$ 
The computation is the same as in VAE.

EM algorithm is an alternation optimization method. Start with an initla guess $\theta^0$.
At step $k$, 
compute two things: 

E-step:

$$
    Q(\theta,\theta^k) := \sum_{i} \mathcal{L}(\theta,p_{\theta^k}(z\mid x^{(i)})\mid x^{(i)}).
$$

M-step:

$$
    \theta^{k+1}\in {\rm arg}\max Q(\theta,\theta^k).
$$

The E-step amounts to choosing the maximizer of $\mathcal{L}(\theta^k,\cdot \mid x^{(i)})$, which is $p_{\theta^k}(z\mid x^{(i)}).$
Note that 

$$
Q(\theta^k,\theta^k)=\ell(\theta^k),\quad 
Q(\theta,\theta^k)\le \ell(\theta).
$$ 

So $Q(\cdot,\theta^k)$ is a lower barrier of $\ell$ at $\theta^k$ (in PDE's term). 

EM always increases likelihood:

$$
    \ell(\theta^{k+1})\ge Q(\theta^{k+1},\theta^{k})
    \ge Q(\theta^k,\theta^k) = \ell(\theta^k). 
$$