---
layout: post
title: "Position Embeddings: Sinusoidal, RoPE"
date: 2025-10-04
math: true
published: true
---

Position embeddings are important in NLP (transformer), CV (diffusion), and others.
The Sinusoidal embedding introduced in [transformer](https://arxiv.org/abs/1706.03762) is quite mysterious and I hope to understand it better.
I partly follow derivations in [RoPE](https://arxiv.org/pdf/2104.09864).

In transformer, say there are $V$ words in total, and $w_i\in \mathbb{R}^D$ is the embedding for word $i$. 
An input of shape $B\times T$ (batch and length) becomes $B\times T\times D$ after embedding and is then fed into attention. 
Now there's nothing special about words' positions, but this info is too important to ignore.

Suppose $D=2d$,
we have an **absolute** position embedding $\phi_t(x)\in \mathbb{C}^d$ for position $t$, and $\phi_t$ is shared by both queries and keys.
We hope the inner products of embeddings should contain **relative** positional infomation.

Given a single query and key $\mathbf{q},\mathbf{k}\in \mathbb{R}^D$, their embeddings should satisfy

$$
    \langle \phi_t(\mathbf{q}), \overline{\phi_s(\mathbf{k})}\rangle = \psi(\mathbf{q},\mathbf{k},t-s)
$$

for some $\psi$.

Naturally, we assume $\phi_0(z)=z.$ 
Then 

$$
\langle \phi_t(z), \overline{\phi_t(w)}\rangle
=\psi(z,w,0)=\langle \phi(z,0), \overline{\phi(w,0)}\rangle=\langle z,\bar w\rangle.
$$

$\phi_t$ is isometry on $\mathbb{C}^n$, and thus $\phi_t(z)=U_tz$ for some unitary matrix $U_t$ (ignoring translation).


$$
U_{t-1}^*U_t=\sum_{i,j}\langle U_t e_j,\overline{U_{t-1}e_i}\rangle e_ie_j^\top 
= \sum_{i,j}\psi(e_j,e_i,1) e_ie_j^\top =:\Theta,
$$

which is independent of $t$.  So,

$$
U_t = U_0^*U_t = (U_0^*U_1)(U_1^*U_2)\cdots(U_{t-1}^*U_t) = \Theta^t.
$$

We are free to choose $\Theta$.  

[RoPE](https://arxiv.org/pdf/2104.09864) uses **diagonal** $U_t.$
Similar to Sinusoidal, RoPE considers angles of the form $\theta^{k/d}$:

$$
    \Theta = {\rm diag}(\mathrm{e}^{\mathrm{i}\theta^{0/d}},
    \mathrm{e}^{\mathrm{i}\theta^{1/d}},\cdots,\mathrm{e}^{\mathrm{i}\theta^{(d-1)/d}}),\quad 
    \phi_t(z)=\Theta^tz
$$

where $\theta=10^{-4}$ or $1/T$ in general, where $T$ is the max horizon/timestep.

For Sinusoidal, $0\le i< d=\frac{D}{2},0\le t<T,$

$$
{\rm PE}(t)_{2i} = \sin(t \theta^{\frac{i}{d}}),\quad 
{\rm PE}(t)_{2i+1} = \cos(t \theta^{\frac{i}{d}}).
$$

In complex form,

$$
    \widetilde{\mathrm{PE}}(t) = \left[\mathrm{e}^{\mathrm{i}t\theta^{0/d}},\cdots,
    \mathrm{e}^{\mathrm{i}t\theta^{(d-1)/d}}\right]
    ={\rm diag}(\Theta^t).
$$

The difference is that in transformer, inputs of attention are token embeddings + position embeddings, while RoPE multiplies $\Theta^t$ with queries and keys. 